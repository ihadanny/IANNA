{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import pickle # as pickle\n",
    "import tensorflow as tf\n",
    "import random\n",
    "OP_NUMBER = 2\n",
    "STATE_INPUT_SIZE = 4\n",
    "HIDDEN_SIZE = 16\n",
    "TOTAL_EPISODES = 30000\n",
    "BATCH_NUMBER = 10\n",
    "gamma = 0.99\n",
    "SUCCESS_GOAL = 180\n",
    "MAX_STEPS = 700\n",
    "\n",
    "#ADAM Optimizer hyper-parameters:\n",
    "LEARNING_RATE = 0.01\n",
    "B1 = 0.8\n",
    "B2= 0.999\n",
    "EPSILON=1e-6\n",
    "\n",
    "OUT_MAX= \"ws.p\"\n",
    "DISPLAY_FREQ=250\n",
    "RENDER_FREQ=False\n",
    "\n",
    "tf.reset_default_graph()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Agent():\n",
    "\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        #Variables:\n",
    "\n",
    "        self.W1 = tf.get_variable(shape=[HIDDEN_SIZE,STATE_INPUT_SIZE],name='w1',\n",
    "                              initializer=tf.contrib.layers.xavier_initializer())\n",
    "        self.W2 = tf.get_variable(shape=[HIDDEN_SIZE,HIDDEN_SIZE],name='w2',\n",
    "                              initializer=tf.contrib.layers.xavier_initializer())\n",
    "        self.W3 = tf.get_variable(shape=[OP_NUMBER,HIDDEN_SIZE],name='w3',\n",
    "                              initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "        self.b1 = tf.get_variable(shape=[HIDDEN_SIZE,1],name='b1',\n",
    "                              initializer=tf.contrib.layers.xavier_initializer())\n",
    "        self.b2 = tf.get_variable(shape=[HIDDEN_SIZE,1],name='b2',\n",
    "                              initializer=tf.contrib.layers.xavier_initializer())\n",
    "        self.b3 = tf.get_variable(shape=[OP_NUMBER,1],name='b3',\n",
    "                              initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "        #Layers:\n",
    "        self.x = tf.placeholder(tf.float32, shape=[STATE_INPUT_SIZE,None],name='x')\n",
    "        self.h1 = tf.tanh(tf.matmul(self.W1,self.x) + self.b1)\n",
    "        self.h2 = tf.tanh(tf.matmul(self.W2,self.h1) + self.b2)\n",
    "        self.y = tf.nn.softmax(tf.matmul(self.W3,self.h2) + self.b3,dim=0)\n",
    "\n",
    "        #Loss function:\n",
    "\n",
    "        self.curr_reward = tf.placeholder(shape=[None],dtype=tf.float32)\n",
    "        self.actions_array = tf.placeholder(shape=[None,2],dtype=tf.int32)\n",
    "        self.pai_array = tf.gather_nd(self.y,self.actions_array)\n",
    "        self.L = -tf.reduce_mean(tf.log(self.pai_array)*self.curr_reward)\n",
    "        self.gradient_holders = []\n",
    "        self.gradients = tf.gradients(self.L,tf.trainable_variables())\n",
    "\n",
    "        #Initialize gradient lists for each trainable variable:\n",
    "        tvars = tf.trainable_variables()\n",
    "        for idx,var in enumerate(tvars):\n",
    "            placeholder = tf.placeholder(tf.float32,name=str(idx)+'_holder')\n",
    "            self.gradient_holders.append(placeholder)\n",
    "\n",
    "        #Update mechanism:\n",
    "        adam = tf.train.AdamOptimizer(learning_rate=LEARNING_RATE,beta1=B1,beta2=B2,epsilon=EPSILON)\n",
    "        self.update_batch = adam.apply_gradients(zip(self.gradient_holders,tf.trainable_variables()))\n",
    "        \n",
    "\n",
    "    def start(self,sess):\n",
    "        \"\"\"\n",
    "        Agent initialization:\n",
    "        Initiazlie the gradient buffer for each \"trainable variable\" \n",
    "        \"\"\"\n",
    "        init = tf.global_variables_initializer()\n",
    "        sess.run(init)\n",
    "        self.grad_buffer = sess.run(tf.trainable_variables())\n",
    "        for ix,grad in enumerate(self.grad_buffer):\n",
    "            self.grad_buffer[ix] = grad * 0\n",
    "\n",
    "        \n",
    "\n",
    "    def get_action(self,sess,observation):\n",
    "        \"\"\"\n",
    "        Given an observation, return action probabilities (the NN output): \n",
    "        \"\"\"\n",
    "        a_dist = sess.run(self.y,feed_dict={self.x:np.reshape(observation,(STATE_INPUT_SIZE,1))})\n",
    "        res = sess.run(self.W1,feed_dict={self.x:np.reshape(observation,(STATE_INPUT_SIZE,1))})\n",
    "        r = np.random.rand()\n",
    "        a = np.random.choice(range(OP_NUMBER),p=a_dist.reshape((OP_NUMBER)))\n",
    "        return a\n",
    "\n",
    "    def train(self,sess,states_array,actions_array,curr_reward):\n",
    "        \"\"\"\n",
    "        NN training procedure: Given arrays of states(observations),\n",
    "        actions and rewards it computes the derivatives of the loss function\n",
    "        then add the derivation values to the buffer, \n",
    "        \"\"\"\n",
    "\n",
    "        G = sess.run(self.gradients,feed_dict={self.x:states_array,self.actions_array:actions_array,self.curr_reward:curr_reward})\n",
    "        for idx,grad in enumerate(G):\n",
    "            self.grad_buffer[idx] += grad\n",
    "        \n",
    "\n",
    "    def update(self,sess):\n",
    "        \"\"\"\n",
    "        NN update procedure: apply the gradients to the NN variables\n",
    "        \"\"\"\n",
    "        feed_dict = dict(zip(self.gradient_holders, self.grad_buffer))\n",
    "     \n",
    "        _ = sess.run(self.update_batch, feed_dict=feed_dict)\n",
    "        for ix,grad in enumerate(self.grad_buffer):\n",
    "            self.grad_buffer[ix] = grad * 0\n",
    "\n",
    "    def save(self,sess,path):\n",
    "        outfile = open(path,\"wb\")\n",
    "        param = sess.run([self.W1,self.b1,self.W2,self.b2,self.W3,self.b3])\n",
    "        pickle.dump(param,outfile)\n",
    "        outfile.close()\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def discount_rewards(arr):\n",
    "    \"\"\"\n",
    "    Helper function for computing discounted rewards,\n",
    "    then the delayed rewards are normalized by the mean and std as requested.\n",
    "    \"\"\"\n",
    "    discounts = np.zeros_like(arr)\n",
    "    reward = 0\n",
    "    for i in reversed(range(arr.size)):\n",
    "        reward=gamma*(arr[i]+reward)\n",
    "        discounts[i] = reward\n",
    "    mean = np.mean(discounts,keepdims=True)    \n",
    "    discounts = discounts - mean\n",
    "    discounts = discounts/ np.std(discounts)\n",
    "    return discounts\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-08-15 23:57:22,032] Making new env: CartPole-v0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "env_d = 'CartPole-v0'\n",
    "env = gym.make(env_d)\n",
    "total_episodes=TOTAL_EPISODES\n",
    "batch_number = BATCH_NUMBER\n",
    "agent = Agent()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "agent.start(sess)\n",
    "episode_number = 0\n",
    "\n",
    "ep_history = []\n",
    "step_num=0\n",
    "total_reward=0\n",
    "rewards = []\n",
    "steps=[]\n",
    "max_reward=0\n",
    "\n",
    "obsrv = env.reset() # Start the 1st game, recieving the 1st state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "done = False\n",
    "\n",
    "while not done and step_num<MAX_STEPS:\n",
    "    step_num+=1\n",
    "    action = agent.get_action(sess,obsrv)\n",
    "    obsrv1, reward, done, info = env.step(action)\n",
    "\n",
    "    total_reward+=reward\n",
    "    ep_history.append((np.array(obsrv),action,reward))\n",
    "    obsrv=obsrv1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(array([ 0.04099542,  0.01682624,  0.04236995, -0.00939053]), 0, 1.0),\n",
       " (array([ 0.04133195, -0.17887694,  0.04218214,  0.29635381]), 0, 1.0),\n",
       " (array([ 0.03775441, -0.37457405,  0.04810921,  0.60203623]), 1, 1.0),\n",
       " (array([ 0.03026293, -0.18015689,  0.06014994,  0.32488656]), 0, 1.0),\n",
       " (array([ 0.02665979, -0.3760814 ,  0.06664767,  0.63591527]), 0, 1.0),\n",
       " (array([ 0.01913816, -0.57206647,  0.07936597,  0.94882011]), 0, 1.0),\n",
       " (array([ 0.00769683, -0.76816205,  0.09834238,  1.26534667]), 0, 1.0),\n",
       " (array([-0.00766641, -0.96439351,  0.12364931,  1.58713775]), 0, 1.0),\n",
       " (array([-0.02695428, -1.1607491 ,  0.15539206,  1.91568275]), 0, 1.0),\n",
       " (array([-0.05016926, -1.35716553,  0.19370572,  2.25225907]), 0, 1.0)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ep_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "episode_number+=1\n",
    "ep_history= np.array(ep_history)               \n",
    "ep_history[:,2] = discount_rewards(ep_history[:,2])\n",
    "episode_number += 1\n",
    "obsrv = env.reset()\n",
    "\n",
    "\"\"\"\n",
    "perform the training step, \n",
    "feeding the network with the ep_history that contains\n",
    "the states,actions, and discounted rewards\n",
    "\"\"\"\n",
    "L=agent.train(sess,np.vstack(ep_history[:,0]).T,\n",
    "    np.dstack((ep_history[:,1].T,np.array(range(step_num))))[0],\n",
    "    ep_history[:,2].T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.04099542,  0.04133195,  0.03775441,  0.03026293,  0.02665979,\n",
       "         0.01913816,  0.00769683, -0.00766641, -0.02695428, -0.05016926],\n",
       "       [ 0.01682624, -0.17887694, -0.37457405, -0.18015689, -0.3760814 ,\n",
       "        -0.57206647, -0.76816205, -0.96439351, -1.1607491 , -1.35716553],\n",
       "       [ 0.04236995,  0.04218214,  0.04810921,  0.06014994,  0.06664767,\n",
       "         0.07936597,  0.09834238,  0.12364931,  0.15539206,  0.19370572],\n",
       "       [-0.00939053,  0.29635381,  0.60203623,  0.32488656,  0.63591527,\n",
       "         0.94882011,  1.26534667,  1.58713775,  1.91568275,  2.25225907]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states_array = np.vstack(ep_history[:,0]).T\n",
    "states_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0],\n",
       "       [0, 1],\n",
       "       [1, 2],\n",
       "       [0, 3],\n",
       "       [0, 4],\n",
       "       [0, 5],\n",
       "       [0, 6],\n",
       "       [0, 7],\n",
       "       [0, 8],\n",
       "       [0, 9]], dtype=object)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions_array = np.dstack((ep_history[:,1].T,np.array(range(step_num))))[0]\n",
    "actions_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.5457281502797957, 1.211399621726729, 0.8736940373296919,\n",
       "       0.5325772854134928, 0.1880149097405646, -0.16002789396946407,\n",
       "       -0.5115862815553514, -0.8666957639653387, -1.2253922108441135,\n",
       "       -1.5877118541560076], dtype=object)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "curr_reward = ep_history[:,2].T\n",
    "curr_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.75403732,  0.79626626,  0.83186954,  0.7947346 ,  0.83072323,\n",
       "         0.85913491,  0.88004708,  0.89437604,  0.90350097,  0.90882754],\n",
       "       [ 0.24596262,  0.20373371,  0.16813044,  0.20526536,  0.16927676,\n",
       "         0.14086504,  0.11995293,  0.10562398,  0.09649906,  0.09117244]], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(agent.y,feed_dict={agent.x:states_array,agent.actions_array:actions_array,agent.curr_reward:curr_reward})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.75403732,  0.79626626,  0.16813044,  0.7947346 ,  0.83072323,\n",
       "        0.85913491,  0.88004708,  0.89437604,  0.90350097,  0.90882754], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(agent.pai_array,feed_dict={agent.x:states_array,agent.actions_array:actions_array,agent.curr_reward:curr_reward})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
