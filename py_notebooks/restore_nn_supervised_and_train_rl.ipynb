{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from gym_ianna.envs.ianna_env import IANNAEnv\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env_d = 'IANNA-v0'\n",
    "#env_d = 'CartPole-v0'\n",
    "\n",
    "#env params\n",
    "if env_d == 'IANNA-v0':\n",
    "    OP_NUMBER = 15\n",
    "    STATE_INPUT_SIZE = 16\n",
    "    MAX_STEPS = 7\n",
    "\n",
    "if env_d == 'CartPole-v0':\n",
    "    OP_NUMBER = 2\n",
    "    STATE_INPUT_SIZE = 4\n",
    "    MAX_STEPS = 200\n",
    "\n",
    "#nn params\n",
    "HIDDEN_SIZE = 20\n",
    "\n",
    "#discount rewards params\n",
    "GAMMA = 0.99\n",
    "\n",
    "#ADAM Optimizer hyper-parameters:\n",
    "LEARNING_RATE = 0.01\n",
    "B1 = 0.8\n",
    "B2= 0.999\n",
    "EPSILON=1e-6\n",
    "\n",
    "#learning params\n",
    "TOTAL_EPISODES = 10000\n",
    "BATCH_NUMBER = 20\n",
    "DISPLAY_FREQ = 1000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Initializing \n",
    "tf.reset_default_graph()\n",
    "\n",
    "W1 = tf.get_variable(shape=[HIDDEN_SIZE,STATE_INPUT_SIZE],name='w1',\n",
    "                      initializer=tf.contrib.layers.xavier_initializer())\n",
    "W2 = tf.get_variable(shape=[HIDDEN_SIZE,HIDDEN_SIZE],name='w2',\n",
    "                      initializer=tf.contrib.layers.xavier_initializer())\n",
    "W3 = tf.get_variable(shape=[OP_NUMBER,HIDDEN_SIZE],name='w3',\n",
    "                      initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "b1 = tf.get_variable(shape=[HIDDEN_SIZE,1],name='b1',\n",
    "                      initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.get_variable(shape=[HIDDEN_SIZE,1],name='b2',\n",
    "                      initializer=tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.get_variable(shape=[OP_NUMBER,1],name='b3',\n",
    "                      initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "#Layers:\n",
    "x = tf.placeholder(tf.float32, shape=[STATE_INPUT_SIZE,None],name='x')\n",
    "h1 = tf.tanh(tf.matmul(W1,x) + b1)\n",
    "h2 = tf.tanh(tf.matmul(W2,h1) + b2)\n",
    "y = tf.nn.softmax(tf.matmul(W3,h2) + b3,dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()\n",
    "sess = tf.InteractiveSession()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ihadanny/anaconda2/envs/py3k/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "#Loss function:\n",
    "curr_reward = tf.placeholder(shape=[None],dtype=tf.float32)\n",
    "actions_array = tf.placeholder(shape=[None],dtype=tf.int32)\n",
    "pai_array = tf.gather(y,actions_array)\n",
    "L = -tf.reduce_mean(tf.log(pai_array)*curr_reward)\n",
    "gradient_holders = []\n",
    "gradients = tf.gradients(L,tf.trainable_variables())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tvars = tf.trainable_variables()\n",
    "#Initialize gradient lists for each trainable variable:\n",
    "for idx,var in enumerate(tvars):\n",
    "    placeholder = tf.placeholder(tf.float32,name=str(idx)+'_holder')\n",
    "    gradient_holders.append(placeholder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#Update mechanism:\n",
    "adam = tf.train.AdamOptimizer(learning_rate=LEARNING_RATE,beta1=B1,beta2=B2,epsilon=EPSILON)\n",
    "update_batch = adam.apply_gradients(zip(gradient_holders,tvars))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# grad buffer is initialized to all zeros. \n",
    "# It's used to accumulate the gradients and is a regular variable, NOT a tf variable\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "#saver.restore(sess, \"../models/ianna-nn-supervised\")\n",
    "\n",
    "grad_buffer = sess.run(tf.trainable_variables())\n",
    "\n",
    "def reset_grad_buffer():\n",
    "    for ix,grad in enumerate(grad_buffer):\n",
    "        grad_buffer[ix] = grad * 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_action(sess,observation):\n",
    "    \"\"\"\n",
    "    Given an observation, return action sampled according to the probabilities of the NN output\n",
    "    \"\"\"\n",
    "    a_dist = sess.run(y,feed_dict={x:np.reshape(observation,(STATE_INPUT_SIZE, 1))})\n",
    "    a = np.random.choice(range(OP_NUMBER),p=a_dist.reshape((OP_NUMBER)))        \n",
    "    return a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(sess,cur_states_array,cur_actions_array,cur_curr_reward):\n",
    "    \"\"\"\n",
    "    NN training procedure: Given arrays of states(observations),\n",
    "    actions and rewards it computes the derivatives of the loss function\n",
    "    then add the derivation values to the buffer \n",
    "    \"\"\"\n",
    "\n",
    "    G = sess.run(gradients,feed_dict={x:cur_states_array,actions_array:cur_actions_array,curr_reward:cur_curr_reward})\n",
    "    for idx,grad in enumerate(G):\n",
    "        grad_buffer[idx] += grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update(sess):\n",
    "    \"\"\"\n",
    "    NN update procedure: apply the gradients to the NN variables\n",
    "    \"\"\"\n",
    "    feed_dict = dict(zip(gradient_holders, grad_buffer))\n",
    "    _ = sess.run(update_batch, feed_dict=feed_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#        IANNA actions would be:\n",
    "#        0) action_type:            back[0], filter[1], group[2]\n",
    "#        1) col_id:                 [0..num_of_columns-1]\n",
    "#        2) filter_operator:        LT[0], GT[1] if the selected column was numeric (maybe change semantics if column is STR?)\n",
    "#        3) filter_decile:          [0..9] the filter operand  \n",
    "#        4) aggregation column_id:  [0..num_of_columns - 1] (what do we do if the selected col is also grouped_by?)\n",
    "#        5) aggregation type:       MEAN[0], COUNT[1], SUM[2], MIN[3], MAX[4]\n",
    "\n",
    "def build_ianna_action_from_grouped_by_field(grouped_by_field):\n",
    "    action = [2, grouped_by_field, 0, 0, 0, 0]\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def project_state_to_nn_input(x):\n",
    "    return x[-STATE_INPUT_SIZE:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def discount_rewards(arr):\n",
    "    \"\"\"\n",
    "    Helper function for computing discounted rewards,\n",
    "    then the delayed rewards are normalized by the mean and std as requested.\n",
    "    \"\"\"\n",
    "    discounts = np.zeros_like(arr)\n",
    "    reward = 0\n",
    "    for i in reversed(range(arr.size)):\n",
    "        reward=GAMMA*(arr[i]+reward)\n",
    "        discounts[i] = reward\n",
    "    # following 3 lines destroy everything when the game is really simple: \n",
    "    # pick 4 fields out of 5 without repeating yourself\n",
    "    #mean = np.mean(discounts,keepdims=True)\n",
    "    #discounts = discounts - mean\n",
    "    #discounts = discounts/ np.std(discounts)\n",
    "    return discounts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-09-04 17:09:53,695] Making new env: IANNA-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading input /home/ihadanny/src/IANNA/gym_ianna/envs/../../data/1.tsv\n",
      "observation space from [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] to [8648 8648   10    2    2    3 8148    0 3692  180   10 8648    1  118  157\n",
      "  208  206    1    1    1    1    1    1    1    1    1    1    1    1    1\n",
      "    1    1] shape (32,)\n",
      "action space MultiDiscrete6\n"
     ]
    }
   ],
   "source": [
    "\n",
    "env = gym.make(env_d)\n",
    "total_episodes=TOTAL_EPISODES\n",
    "batch_number = BATCH_NUMBER\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "episode_number = 0\n",
    "\n",
    "rewards = []\n",
    "steps=[]\n",
    "max_reward=0\n",
    "\n",
    "reset_grad_buffer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latest game actions:  [3 3 6 0 13 4 10]\n",
      "latest game reward:  -4.0\n",
      "latest game first state:  [206   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      "latest game last state:  [206   1   0   0   1   1   0   1   0   0   0   1   0   0   1   0]\n",
      "Total episodes: 1000\n",
      "Average steps: 7.000000\n",
      "Average reward: -9.907000\n",
      "latest game actions:  [5 6 3 1 8 2 9]\n",
      "latest game reward:  7.0\n",
      "latest game first state:  [206   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      "latest game last state:  [206   0   1   1   1   0   1   1   0   1   1   0   0   0   0   0]\n",
      "Total episodes: 2000\n",
      "Average steps: 7.000000\n",
      "Average reward: -9.335000\n",
      "latest game actions:  [13 1 3 1 6 7 2]\n",
      "latest game reward:  -4.0\n",
      "latest game first state:  [206   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      "latest game last state:  [206   0   1   1   1   0   0   1   1   0   0   0   0   0   1   0]\n",
      "Total episodes: 3000\n",
      "Average steps: 7.000000\n",
      "Average reward: -8.125000\n",
      "latest game actions:  [2 6 14 14 8 4 6]\n",
      "latest game reward:  -15.0\n",
      "latest game first state:  [206   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      "latest game last state:  [206   0   0   1   0   1   0   1   0   1   0   0   0   0   0   1]\n",
      "Total episodes: 4000\n",
      "Average steps: 7.000000\n",
      "Average reward: -8.763000\n",
      "latest game actions:  [7 10 11 8 11 10 3]\n",
      "latest game reward:  -15.0\n",
      "latest game first state:  [206   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      "latest game last state:  [206   0   0   0   1   0   0   0   1   1   0   1   1   0   0   0]\n",
      "Total episodes: 5000\n",
      "Average steps: 7.000000\n",
      "Average reward: -7.839000\n",
      "latest game actions:  [3 3 12 1 6 3 0]\n",
      "latest game reward:  -15.0\n",
      "latest game first state:  [206   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      "latest game last state:  [206   1   1   0   1   0   0   1   0   0   0   0   0   1   0   0]\n",
      "Total episodes: 6000\n",
      "Average steps: 7.000000\n",
      "Average reward: -8.323000\n",
      "latest game actions:  [10 6 7 7 3 9 4]\n",
      "latest game reward:  -4.0\n",
      "latest game first state:  [206   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      "latest game last state:  [206   0   0   0   1   1   0   1   1   0   1   1   0   0   0   0]\n",
      "Total episodes: 7000\n",
      "Average steps: 7.000000\n",
      "Average reward: -7.916000\n",
      "latest game actions:  [9 7 1 10 1 0 0]\n",
      "latest game reward:  -15.0\n",
      "latest game first state:  [206   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      "latest game last state:  [206   1   1   0   0   0   0   0   1   0   1   1   0   0   0   0]\n",
      "Total episodes: 8000\n",
      "Average steps: 7.000000\n",
      "Average reward: -8.807000\n",
      "latest game actions:  [5 5 4 0 14 5 1]\n",
      "latest game reward:  -15.0\n",
      "latest game first state:  [206   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      "latest game last state:  [206   1   1   0   0   1   1   0   0   0   0   0   0   0   0   1]\n",
      "Total episodes: 9000\n",
      "Average steps: 7.000000\n",
      "Average reward: -8.983000\n",
      "latest game actions:  [2 1 4 12 4 5 12]\n",
      "latest game reward:  -15.0\n",
      "latest game first state:  [206   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      "latest game last state:  [206   0   1   1   0   1   1   0   0   0   0   0   0   1   0   0]\n",
      "Total episodes: 10000\n",
      "Average steps: 7.000000\n",
      "Average reward: -9.577000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "while episode_number <= total_episodes:\n",
    "    for ep in range(batch_number):\n",
    "        obsrv = project_state_to_nn_input(env.reset())\n",
    "        ep_history=[]\n",
    "        step_num=0\n",
    "        total_reward=0\n",
    "        done=False\n",
    "\n",
    "        while not done and step_num < MAX_STEPS:\n",
    "            #Perform the game \"step:\"\n",
    "            step_num+=1\n",
    "            action = get_action(sess,obsrv)\n",
    "            if env_d == 'IANNA-v0':\n",
    "                complex_action = build_ianna_action_from_grouped_by_field(action)\n",
    "                obsrv1, reward, done, info = env.step(complex_action)\n",
    "            else:\n",
    "                obsrv1, reward, done, info = env.step(action)\n",
    "\n",
    "            total_reward+=reward\n",
    "            ep_history.append((np.array(obsrv),action,reward))\n",
    "            obsrv=project_state_to_nn_input(obsrv1)\n",
    "\n",
    "        episode_number+=1\n",
    "        ep_history= np.array(ep_history)   \n",
    "        ep_history[:,2] = discount_rewards(ep_history[:,2])\n",
    "\n",
    "        \"\"\"\n",
    "        perform the training step, \n",
    "        feeding the network with the ep_history that contains\n",
    "        the states,actions, and discounted rewards\n",
    "        \"\"\"\n",
    "        ep_states_array = np.vstack(ep_history[:,0]).T\n",
    "        ep_actions_array = ep_history[:,1].T\n",
    "        ep_curr_reward = ep_history[:,2].T\n",
    "        L=train(sess, ep_states_array, ep_actions_array, ep_curr_reward)\n",
    "\n",
    "        #update the rewards/steps counter, storing the data for all episodes\n",
    "        rewards.append(total_reward)\n",
    "        steps.append(step_num)    \n",
    "        \n",
    "        if episode_number%DISPLAY_FREQ==0:\n",
    "            print(\"latest game actions: \", ep_actions_array.T)\n",
    "            print(\"latest game reward: \", total_reward)\n",
    "            print(\"latest game first state: \", ep_states_array.T[0])\n",
    "            print(\"latest game last state: \", obsrv)\n",
    "            print(\"Total episodes: %d\"%episode_number)\n",
    "            print(\"Average steps: %f\"%np.mean(steps[-DISPLAY_FREQ:]))\n",
    "            print(\"Average reward: %f\"%np.mean(rewards[-DISPLAY_FREQ:]))\n",
    "    update(sess)\n",
    "    reset_grad_buffer()\n",
    "    if np.mean(rewards[-batch_number:])>max_reward:\n",
    "        max_reward=np.mean(rewards[-batch_number:])\n",
    "        print(\"\\t\\t\\tCurr Max mean reward:\",max_reward)\n",
    "  \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#sess.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
