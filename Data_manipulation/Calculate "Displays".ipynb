{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import pyreact_core.query.displays as qd\n",
    "import pyreact.query.packets as qp\n",
    "import operator\n",
    "import json\n",
    "import numpy as np\n",
    "import scipy as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#init:\n",
    "actions = pd.read_csv('experts_dataset/experts_actions.tsv', sep = '\\t', escapechar='\\\\')\n",
    "displays = pd.read_csv('experts_dataset/experts_displays.tsv', sep = '\\t', escapechar='\\\\')\n",
    "data=[]\n",
    "for i in range(4):\n",
    "    df = pd.read_csv('raw_datasets/'+str(i+1)+\".tsv\", sep = '\\t', index_col=0)\n",
    "    data.append(df)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def hack_min(pd_series):\n",
    "    return np.min(pd_series.dropna())\n",
    "def hack_max(pd_series):\n",
    "    return np.max(pd_series.dropna())\n",
    "\n",
    "\n",
    "INT_OPERATOR_MAP = {\n",
    "    8: operator.eq,\n",
    "    32: operator.gt,\n",
    "    64: operator.ge,\n",
    "    128: operator.lt,\n",
    "    256: operator.le,\n",
    "    512: operator.ne,\n",
    "}\n",
    "\n",
    "AGG_MAP = {\n",
    "    'sum': np.sum,\n",
    "    'count': len ,\n",
    "    'min': hack_min,#lambda x:np.nanmin(x.dropna()),\n",
    "    'max': hack_max,#lambda x:np.nanmax(x.dropna()),\n",
    "    'avg': np.mean\n",
    "}\n",
    "\n",
    "KEYS=[ 'eth_dst', 'eth_src', 'highest_layer', 'info_line',\n",
    "       'ip_dst', 'ip_src', 'length', 'number',\n",
    "        'sniff_timestamp', 'tcp_dstport', 'tcp_srcport',\n",
    "       'tcp_stream']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_filtered_df(project_id,filtering_dict):\n",
    "    #Given a project_id and the filtering critria, return the corresponding DataFrame\n",
    "    #legacy:\n",
    "    filters=filtering_dict[\"list\"]\n",
    "    df=data[project_id-1].copy()\n",
    "    if filters:\n",
    "        for filt in filters:\n",
    "            field = filt[\"field\"]\n",
    "            op_num = filt[\"condition\"]\n",
    "            value = filt[\"term\"]\n",
    "            #print(field,op_num,value)\n",
    "\n",
    "            #extract the operation:\n",
    "            #print(field,op_num,value)\n",
    "            if op_num in INT_OPERATOR_MAP.keys():\n",
    "                opr = INT_OPERATOR_MAP.get(op_num)\n",
    "                value= float(value) if df[field].dtype!='O' else value\n",
    "                df = df[opr(df[field], value)]\n",
    "            else:\n",
    "                if op_num==16:\n",
    "                    df = df[df[field].str.contains(value,na=False)]\n",
    "                if op_num==2:\n",
    "                    df = df[df[field].str.startswith(value,na=False)]\n",
    "                if op_num==4:\n",
    "                    df = df[df[field].str.endswith(value,na=False)]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_groupby_df(df,grouping_dict,aggregation_dict):\n",
    "    #Given a dataframe, the grouping and aggregations - result (i) the aggregated dataframe, and (ii)the groupby element\n",
    " \n",
    "    groupings=grouping_dict[\"list\"]\n",
    "    if aggregation_dict:\n",
    "        aggregations=aggregation_dict[\"list\"]\n",
    "        #print(aggregations)\n",
    "    else:\n",
    "        aggregations = None\n",
    "    grouping_attrs = [group[\"field\"] for group in groupings]\n",
    "    if not grouping_attrs:\n",
    "        return None,None\n",
    "    \n",
    "    df_gb= df.groupby(grouping_attrs)\n",
    "    \n",
    "    agg_dict={'number':len} #all group-by gets the count by default in REACT-UI\n",
    "    if aggregations: #Custom aggregations: sum,count,avg,min,max\n",
    "        for agg in aggregations:\n",
    "            agg_dict[agg['field']] = AGG_MAP.get(agg['type'])\n",
    "\n",
    "        \n",
    "    agg_df = df_gb.agg(agg_dict)\n",
    "    return df_gb,agg_df\n",
    "\n",
    "def get_df_by_row(row):\n",
    "    return get_filtered_df(row[\"project_id\"],json.loads(row[\"filtering\"]))\n",
    "\n",
    "def get_grouping_by_row(row):\n",
    "    df = get_filtered_df(row[\"project_id\"],json.loads(row[\"filtering\"]))\n",
    "    df_gb,agg_df = get_groupby_df(df,json.loads(row[\"grouping\"]),json.loads(row[\"aggregations\"]))\n",
    "    return df_gb, agg_df\n",
    "    \n",
    "\n",
    "    \n",
    "        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###MEASURES:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_data_column_measures(column):\n",
    "    #for each column, compute its: (1) normalized value entropy (2)Null count (3)Unique values count\n",
    "    B=20\n",
    "    u = column.nunique()\n",
    "    n = column.isnull().sum()\n",
    "    column_na=column.dropna()\n",
    "    size=len(column)\n",
    "    if column.dtype=='O':\n",
    "        h=sp.stats.entropy(column_na.value_counts().values)/np.log(len(column.dropna()))\n",
    "    else:\n",
    "        h= sp.stats.entropy(np.histogram(column_na,bins=B)[0])/np.log(B)\n",
    "    return {\"unique\":u/(size-n),\"nulls\":n/size,\"entropy\":h}\n",
    "\n",
    "def calc_data_layer(disp_row):\n",
    "    #This method take a display row, and calculate the \"data layer\" measures for each column\n",
    "    df=get_filtered_df(disp_row[\"project_id\"],json.loads(disp_row[\"filtering\"]))\n",
    "    return df[KEYS].apply(get_data_column_measures).to_dict()\n",
    "\n",
    "def get_grouping_measures(group_obj,agg_df):\n",
    "    \"\"\"\"number\" is the unique identifier of a packet, \n",
    "    therefore we use it to count the size of each group , \n",
    "    although this may feel hacky\"\"\"\n",
    "    if group_obj is None or agg_df is None:\n",
    "        return None \n",
    "    B=20\n",
    "    groups_num=len(group_obj)\n",
    "    size_var=np.var(agg_df.number/np.sum(agg_df.number))\n",
    "    size_mean = np.mean(agg_df.number)\n",
    "    group_keys=group_obj.keys\n",
    "    agg_keys=list(agg_df.keys()).remove(\"number\")\n",
    "    agg_nve_dict={}\n",
    "    if agg_keys is not None:\n",
    "        for ak in agg_keys:\n",
    "            agg_nve_dict[ak]=sp.stats.entropy(np.histogram(agg_df[ak],bins=B)[0])/np.log(B)\n",
    "    return {\"group_attrs\":group_keys,\"agg_attrs\":agg_nve_dict,\"ngroups\":groups_num,\"size_var\":size_var,\"size_mean\":size_mean}\n",
    "    \n",
    "def calc_gran_layer(disp_row):\n",
    "    #this method takes a display row, and calculates the \"granularity layer\" measures\n",
    "    group_obj,agg_df = get_grouping_by_row(disp_row)\n",
    "    return get_grouping_measures(group_obj,agg_df)\n",
    "\n",
    "#    df=get_filtered_df(disp_row[\"project_id\"],json.loads(row[\"filtering\"]))\n",
    "#    return df[KEYS].apply(get_data_column_measures).to_dict()\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
